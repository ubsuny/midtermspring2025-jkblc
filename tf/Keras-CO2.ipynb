{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "offensive-absolute",
      "metadata": {
        "id": "offensive-absolute"
      },
      "source": [
        "# Predict seasonal CO2 concentration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocational-resolution",
      "metadata": {
        "id": "vocational-resolution"
      },
      "source": [
        "Time Series prediction is a useful tool for many areas of physics and society.\n",
        "Here we will use keras to predict the seasonal CO2 concentration change from our previous homework.\n",
        "Note: here we looking into the seasonal change whereas in the previous homework we were looking into the seasonal corrected outlook.\n",
        "\n",
        "Loosly based on [https://machinelearningmastery.com/time-series-prediction-with-deep-learning-in-python-with-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "direct-brush",
      "metadata": {
        "id": "direct-brush"
      },
      "source": [
        "Start with importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "automated-projection",
      "metadata": {
        "id": "automated-projection"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "italian-metabolism",
      "metadata": {
        "id": "italian-metabolism"
      },
      "source": [
        "This code is written for\n",
        "- tensorflow/keras 2.15\n",
        "- Pandas version 2.03\n",
        "- Python version 3.10.12\n",
        "\n",
        "So check your versions first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "middle-struggle",
      "metadata": {
        "id": "middle-struggle"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(\"Python: {}\".format(sys.version))\n",
        "print(\"tensorflow: {}\".format(tf.__version__))\n",
        "print(\"keras: {}\".format(keras.__version__))\n",
        "print(\"pandas: {}\".format(pd.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "subjective-friend",
      "metadata": {
        "id": "subjective-friend"
      },
      "source": [
        "## Data input\n",
        "Let's read in out CO2 data in frist.\n",
        "We skip the header rows and are only interested in the non-season corrected monthly average and set the decimal date as our index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compound-insulin",
      "metadata": {
        "id": "compound-insulin"
      },
      "outputs": [],
      "source": [
        "dfraw = pd.read_fwf('https://github.com/ubsuny/CompPhys/raw/main/DataAnalysis/FFT/co2_mm_mlo.txt',skiprows=71,index_col=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adult-maine",
      "metadata": {
        "id": "adult-maine"
      },
      "source": [
        "Let's filter out all missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "express-remainder",
      "metadata": {
        "id": "express-remainder"
      },
      "outputs": [],
      "source": [
        "df = dfraw.iloc[:,2][dfraw.iloc[:,2]>0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pediatric-pleasure",
      "metadata": {
        "id": "pediatric-pleasure"
      },
      "outputs": [],
      "source": [
        "df.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "independent-executive",
      "metadata": {
        "id": "independent-executive"
      },
      "source": [
        "For our model it is convenient to use normalized data, so we take the mean and divide by the standard derivation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collect-specialist",
      "metadata": {
        "id": "collect-specialist"
      },
      "outputs": [],
      "source": [
        "def normalize(data):\n",
        "    data_mean = data.mean(axis=0)\n",
        "    data_std = data.std(axis=0)\n",
        "    return (data - data_mean) / data_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "educated-familiar",
      "metadata": {
        "id": "educated-familiar"
      },
      "outputs": [],
      "source": [
        "normalize(df).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "limited-proxy",
      "metadata": {
        "id": "limited-proxy"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "developmental-apple",
      "metadata": {
        "id": "developmental-apple"
      },
      "source": [
        "In the next step we split our dataset into a training set and a validation (test) set.\n",
        "This helps us to validate the training against a known future.\n",
        "You can experiment with the split ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gentle-video",
      "metadata": {
        "id": "gentle-video"
      },
      "outputs": [],
      "source": [
        "splitRatio = 0.67\n",
        "train_size = int(len(df) * splitRatio)\n",
        "test_size = len(df) - train_size\n",
        "train = normalize(df).iloc[0:train_size].values\n",
        "test = normalize(df).iloc[train_size:].values\n",
        "print(\"length of training set: {},\\nlength of validation set: {}\".format(len(train), len(test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scenic-editor",
      "metadata": {
        "id": "scenic-editor"
      },
      "source": [
        "Now we create a dataset for our model.\n",
        "There are also built in functions\n",
        "\n",
        "`keras.preprocessing.timeseries_dataset_from_array`\n",
        "\n",
        "[https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array]\n",
        "\n",
        "for that but for educational purposes we do that ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "previous-cosmetic",
      "metadata": {
        "id": "previous-cosmetic"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset, look_back=3):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back)][0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back])\n",
        "    return numpy.array(dataX), numpy.array(dataY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "equipped-string",
      "metadata": {
        "id": "equipped-string"
      },
      "source": [
        "What did we do here?\n",
        "We use a *look_back* variable to define the amount of months we want to use to predict the future.\n",
        "Then we stack up an $n+1$ dimensional array that uses the original timeseries shifted by *look_back*.\n",
        "This gives us the data for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "later-domain",
      "metadata": {
        "id": "later-domain"
      },
      "source": [
        "Let's have a look at it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "facial-throw",
      "metadata": {
        "id": "facial-throw"
      },
      "outputs": [],
      "source": [
        "x,y = create_dataset(df.iloc[0:10].values)\n",
        "dd = pd.DataFrame([x,y])\n",
        "dd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "material-stephen",
      "metadata": {
        "id": "material-stephen"
      },
      "source": [
        "Depending on *look_back* we shifted the timeseries in the following row."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "modern-pierce",
      "metadata": {
        "id": "modern-pierce"
      },
      "source": [
        "Let's do it for our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "behind-kruger",
      "metadata": {
        "id": "behind-kruger"
      },
      "outputs": [],
      "source": [
        "look_back = 1\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "colored-mandate",
      "metadata": {
        "id": "colored-mandate"
      },
      "source": [
        "## Create Model\n",
        "now we create our model.\n",
        "What changes if you use less or more hidden layers?\n",
        "You can also play around with the loss function and the optimizer to see the effects.\n",
        "We use a batch size of zero for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "australian-trunk",
      "metadata": {
        "id": "australian-trunk"
      },
      "outputs": [],
      "source": [
        "with tf.device(\"cpu:0\"):\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(6, input_dim=1, activation='relu'))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    model.fit(trainX, trainY, epochs=150, batch_size=0, verbose=1)\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "extra-jungle",
      "metadata": {
        "id": "extra-jungle"
      },
      "source": [
        "## Estimate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "indonesian-liechtenstein",
      "metadata": {
        "id": "indonesian-liechtenstein"
      },
      "outputs": [],
      "source": [
        "with tf.device(\"cpu:0\"):\n",
        "    trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
        "    print(\"Train Score: {:.2f} MSE\".format(trainScore))\n",
        "    testScore = model.evaluate(testX, testY, verbose=0)\n",
        "    print(\"Test Score: {:.2f} MSE\".format(testScore))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yellow-finnish",
      "metadata": {
        "id": "yellow-finnish"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "forbidden-registration",
      "metadata": {
        "id": "forbidden-registration"
      },
      "source": [
        "Now use the keras predition to validate our data.\n",
        "Is that the expected outcome?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "happy-basket",
      "metadata": {
        "id": "happy-basket"
      },
      "outputs": [],
      "source": [
        "with tf.device(\"cpu:0\"):\n",
        "    # generate predictions for training\n",
        "    trainPredict = model.predict(trainX)\n",
        "    testPredict = model.predict(testX)\n",
        "    # shift train predictions for plotting\n",
        "    trainPredictPlot = np.empty_like(df.iloc[0:train_size].values)\n",
        "    #trainPredictPlot[:,:] = np.nan\n",
        "    trainPredictPlot[look_back:len(trainPredict)+look_back] = trainPredict[:,0]\n",
        "    # shift test predictions for plotting\n",
        "    testPredictPlot = np.empty_like(df)\n",
        "    testPredictPlot[:] = np.nan\n",
        "    testPredictPlot[len(trainPredict)+(look_back*2)+1:len(df)-1] = testPredict[:,0]\n",
        "    # plot baseline and predictions\n",
        "    plt.plot(normalize(df).values, label=\"Normalized\")\n",
        "    plt.plot(trainPredictPlot, label=\"Training Data\")\n",
        "    plt.plot(testPredictPlot, label=\"Predicted Data\")\n",
        "    plt.legend()\n",
        "    ylim(-2,2)\n",
        "    plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "juvenile-tribe",
      "metadata": {
        "id": "juvenile-tribe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}