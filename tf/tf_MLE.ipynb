{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood estimator\n",
    "This notebook shows how to do maximum likelihood estimation (MLE) in tensorflow\n",
    "based on [https://thelongrun.blog/2019/11/24/maximum-likelihood-with-tensorflow-2-0/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to do maximum likelihood estimation (MLE), and for demostration purposes, data will be generated with two distributions. I encourage you try with your data instead.\n",
    "\n",
    "The main libraries we use for this tutorial are Tensorflow and Tensorflow Probability. Just because of the easiness with computing maximum likelihood.\n",
    "\n",
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_func = {'mu': tf.math.reduce_mean, # Computes the mean of elements across dimensions of a tensor.\n",
    "                  'sigma':tf.math.reduce_std, # Computes the standard deviation of elements across dimensions of a tensor.\n",
    "                  'rate': tf.math.reduce_mean}\n",
    "\n",
    "colors_encoding=['b','m','g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to perform MLE\n",
    "\n",
    "\n",
    "# A loss function, although for us, it has to be the likelihood function. \n",
    "# Since optimization in Tensorflow follows a minimization procedure we use the negative \n",
    "# of the log-likelihood function which is equivalent to maximizing the likelihood. \n",
    "# The log operator deters from running into underflow issues when calculating the likelihood of hundreds of samples. \n",
    "# This function will depend on the data and parameters that have been declared.\n",
    "def loss(model, data):\n",
    "    total_log_prob = -tf.reduce_mean(model.log_prob(data))\n",
    "    return total_log_prob\n",
    "    \n",
    "# A function to record the gradients and later use them to update the parameters during the learning process. \n",
    "# These gradients are calculated by differentiation of the loss function with respect to the parameters. \n",
    "# The tape object in Tensorflow takes care of everything.\n",
    "# This process is called Automatic Differentiation\n",
    "def grad(model, inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "\n",
    "# During the training process, the grad function returns the loss and the gradients. \n",
    "# These are then used to update the parameters with the method apply_gradients of the optimizer. \n",
    "# The choice for this example is the Adam optimizer. \n",
    "# We show the loss and the parameter values are updated every 10 iterations. \n",
    "def mle_run(data, model, parameters, optimizer, steps=1000, verbose=False):\n",
    "    update_list = []\n",
    "    prob_values = []\n",
    "    value_range = tf.linspace(tf.reduce_min(data),tf.reduce_max(data), 100)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        loss_value, grads = grad(model, data)\n",
    "        optimizer.apply_gradients(zip(grads, parameters))\n",
    "    \n",
    "        if i % 10 == 0:\n",
    "            update_list.append((\n",
    "                optimizer.iterations.numpy(),loss_value.numpy(), \n",
    "                *[p.numpy()[0] for p in parameters]))\n",
    "            param_str = \", \".join([p.name.split(':')[0]+\": \"+str(p.numpy()) for p in parameters])\n",
    "            iter_info = f\"Step: {optimizer.iterations.numpy()}, initial loss: {loss_value.numpy()}, {param_str}\"\n",
    "            if verbose:\n",
    "                print(iter_info)\n",
    "            prob_values.append(model.prob(value_range))\n",
    "    \n",
    "    return update_list, prob_values, value_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for plotting\n",
    "# Code adapted from: https://alexgude.com/blog/matplotlib-blitting-supernova/\n",
    "%matplotlib inline\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "from matplotlib import animation\n",
    "\n",
    "def init_fig(f, ax, artists, data):\n",
    "    \n",
    "    freq, bins, _ = plt.hist(data.numpy(), bins=50, density = True);\n",
    "    # Set axis and plot titles\n",
    "    ax.set_title(\"Fitting data with Maximum Likelihood\", fontsize=20)\n",
    "    ax.set_xlabel(\"Values of the distribution\", fontsize=14)\n",
    "    ax.set_ylabel(r\"$p(D|\\theta)$\", fontsize=14)\n",
    "    \n",
    "    ax.set_xlim((tf.reduce_min(data)-0.1*tf.math.abs(tf.reduce_min(data))).numpy(), \n",
    "                (tf.reduce_max(data)+0.1*tf.math.abs(tf.reduce_max(data))).numpy()\n",
    "               )\n",
    "    ax.set_ylim(0.,freq.max())\n",
    "    \n",
    "    \n",
    "    return artists\n",
    "\n",
    "def frame_iter(update_list, prob_values):\n",
    "    for i in range(len(prob_values)):\n",
    "        yield (update_list[i][0], prob_values[i])\n",
    "        \n",
    "def update_artists(frames, artists, value_range):\n",
    "    s, p = frames\n",
    "    artists.prob.set_data(value_range.numpy(), p)\n",
    "    artists.step.set_text(\"Step \" + str(s))\n",
    "\n",
    "# Function to plot learning curves\n",
    "def plot_curves(update_lists, data, params_list):\n",
    "    from pandas import DataFrame\n",
    "    param_names = [p.name.split(':')[0] for p in params_list]\n",
    "    learning_df = DataFrame(update_lists, columns=[\"Step\", \"Loss\", *param_names])\n",
    "    \n",
    "    f, ax = plt.subplots(1+len(params_list), 1, sharex = True, figsize=(16, 8))\n",
    "    learning_df.plot(x = \"Step\", y='Loss',style=\"r--X\" ,ax=ax[0])\n",
    "    for i, p in enumerate(param_names):\n",
    "        learning_df.plot(x = \"Step\", y=p, style=f\"{colors_encoding[i]}--*\",linewidth=3,ax=ax[i+1])\n",
    "        ax[i+1].axhline(params_to_func[p](data).numpy(), color=colors_encoding[i])\n",
    "        ax[i+1].legend([f\"MLE {p}\",f\"True {p}\"])\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE to Normal random variable\n",
    "The first example generates data sampling from a Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "x = 0.66+tf.random.normal([5000])\n",
    "# Two variables\n",
    "mu = tf.Variable([0.], name=\"mu\") # Mean of the distribution\n",
    "sigma = tf.Variable([3.], name=\"sigma\") # Standard deviation of the distribution\n",
    "\n",
    "# The model is a probability distribution\n",
    "model = tfd.Normal(loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(0.01)\n",
    "ul, pv, vr = mle_run(x, model, [mu, sigma], optimizer, steps=300, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting an animation of updates in the probability distribution fitted by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(18,10))\n",
    "Artists = namedtuple(\"Artists\",(\"prob\",\"step\"))\n",
    "artists = Artists(plt.plot([], [], \"r--\",animated=True)[0], \n",
    "                  ax.text(x=0.9*vr[-1].numpy(), y = 0.05, s=\"\"))\n",
    "\n",
    "init = partial(init_fig, f=f, ax=ax, artists=artists, data=x)\n",
    "update = partial(update_artists, artists=artists, value_range=vr)\n",
    "frame = partial(frame_iter, update_list=ul, prob_values=pv)\n",
    "\n",
    "ani = animation.FuncAnimation(fig=f, \n",
    "                              func=update, \n",
    "                              frames = frame,\n",
    "                              init_func = init)\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the learning curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(ul, x, [mu,sigma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE to Poisson random variable\n",
    "The second example tries to estimate the parameter from data generated with a discrete distribution like Poisson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_samples = tf.random.poisson([300], 0.5)\n",
    "fake_samples = tf.cast(tf.math.ceil(fake_samples), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is a probability distribution\n",
    "rate = tf.Variable([5.], name=\"rate\") # Give the parameter an initial value\n",
    "model = tfd.Poisson(rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(0.01)\n",
    "ulp, pvp, vrp = mle_run(fake_samples, model, [rate], optimizer, steps=800, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15,10))\n",
    "Artists = namedtuple(\"Artists\",(\"prob\",\"step\"))\n",
    "artists = Artists(plt.plot([], [], \"r--\",animated=True)[0], \n",
    "                  ax.text(x=0.9*vrp[-1].numpy(), y = 0.05, s=\"\"))\n",
    "\n",
    "init = partial(init_fig, f=f, ax=ax, artists=artists, data=fake_samples)\n",
    "update = partial(update_artists, artists=artists, value_range=vrp)\n",
    "frame = partial(frame_iter, update_list=ulp, prob_values=pvp)\n",
    "\n",
    "ani = animation.FuncAnimation(fig=f, \n",
    "                              func=update, \n",
    "                              frames = frame,\n",
    "                              init_func = init)\n",
    "\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot the learning curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_curves(ulp, fake_samples, [rate])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
